---
title: 'AI Part 1: LLM Basics'
date: 2025-03-05T17:21:27-08:00
description: "The ultimate intro to AI you've never had"
minread: 30
comments: true
draft: false
tags:
 - Neural Networks
---

## Introduction

When we think of computers, we sort of know how they work. We know that there is the hardware, the operating system, and then the compiled applications. If you're an avid programmer, you might even know how memory works, what header files and pre-compiled libraries are, how to write a program, etcetera. The point is, you know how and why the output of these systems are the way they are. 

AI models, on the other hand, are different. When you input something to an AI model, the outputs do not seem deterministic to the end user. This can pose significant reliability and trust issues when an AI model is used. Today, the rise of the Large Language Models (LLMs) that can seemingly pass the [Turing Test](https://medium.com/@kyledavidstephens/what-is-the-turing-test-anyway-and-does-it-still-matter-6edc3f4687cb), the question of how the models are achieving near-human levels of performance is ever more relevant. This problem is especially magnified when many resources online don't cover the entire picture of AI. For example, when you try to research online about how LLMs work, you will often either find basic machine learning algorithms such as linear regression algorithms without expanding on how it connects to LLMs, or find technical deep dives into architectures such as Transformers that assumes in-depth prior knowledge. Very rarely will you find a comprehensive article explaining AI that isn't too basic or too technical. 

Well, that was my experience trying to get a grasp in how AI worked. Fortunately for you, after digging and banging my head through multitudes of articles online, I figured out exactly what the AI industry have been hiding from us. In this article, I will explain clearly and (hopefully) accurately, how in the world do LLMs today function. I will try to make this article approachable to people with no prior experience in AI or math; in other words, people like me. So this article won't go too deep into the technical details. Therefore, as long as you have some high school level math under your belt, it should hopefully be fairly easy to understand everything. Note that because of this, some parts may be a gross oversimplification of the real working deal. 

## Terminology

First, let's begin by defining and distinguishing between some terms: **artificial intelligence**, **machine learning**, and **deep learning**. Note that the defined boundaries of these fields are somewhat vague, however it is still useful to know the difference between them. In most contexts, the terms each describe the following fields:

+ **Artificial Intelligence (AI)** describes machines and computer programs that appear intelligent. This term encompasses any computer program that has the ability to solve tasks that would normally require humans. It is a very general term that encompasses a large field. 
+ **Machine Learning (ML)** describes machines that can "self-learn" from the data with simple ML algorithms using statistics (for example, regression algorithms). They are generally only able to find patterns in simple and structured data. This is a subfield of artificial intelligence.
+ **Deep Learning (DL)** is machine learning with (multi-layered) neural networks. They are able to grasp patterns from complex unstructured data. These models are the ones that produce the best results and achieve human-level performance. This is a subfield of machine learning.

Because AI describes such a broad field, it doesn't really concern our purpose in this blog with explaining how LLMs work. Therefore, I will jump directly into machine learning using statistics, then explore deep learning and neural networks, before finally discussing how LLMs connect to all of this. 

## Machine Learning

#### 1. Linear Regression

Let's first jump into ML using statistics. One of the simplest machine learning model to train are regression models. A regression model calculates the function of best fit to represent the relationship between input and output data. If we have one input variable and one output variable, we can use regression models to calculate the general trend of this data. Then, we can use this trend to anticipate what the output should be for an unseen input data, based on known inputs. 

For example, the **simple linear regression** model calculates the **line of best fit**. A fictional application for this could be if we want to determine a tumor's rate of growth from its hormonal activity. We can first collect some real-world data of the correlation between a tumor's rate of growth and its hormonal activity. Then, we can calculate the linear regression, or line of best fit, for the data using statistics. When we encounter hormonal data that we haven't seen before, we could estimate the rate at which the tumor grows based on our line of best fit. The more data that we have, the more accurate the model gets at forecasting the growth rate of the tumor. Graphically, the resulting model would look something like the following:

![Visual of linear regression](/assets/02-linear-regression.jpeg)

#### 2. Logistic Regression

So far, a linear regression model is working fine for our purposes of determining how one **continuous** value affect another. In our example above, both the hormonal activity and the growth rate lies on a continuous spectrum of values, so the linear regression models the data well. 

However, if we encounter something that requires us to determine a **binary** value (yes or no) from a continuous function, linear regression performs inadequately. This can be illustrated with another fictional example: say we want to determine whether or not a tumor is cancerous (binary function) by its rate of growth (continuous function). We will plot the rate of growth on the x-axis, and the probability of whether or not a tumor is cancerous on the y-axis. The following graph shows the prediction of a linear regression model:

![Visual of linear regression (binary)](/assets/02-linear-regression-binary.jpeg)

There are several issues with this approach. Notice above that the "line of best fit" doesn't really fit the data well: the predicted output values exceed 0 and 1. There is also a large section in the middle of the input data where the output values are close to 0.5. In other words, we aren't really confident with the output. 

To remediate these issues, we can turn to **logistic regression**, where we use more math to squish the output data into a logistic curve between 0 and 1. We will skip over the math here, however the end result would be similar to the following image: 

![Visual of logistic regression (binary)](/assets/02-logistic-regression-binary.jpeg)

As we can see, the regression line fits the data much better. In addition, the predicted output values do not exceed 0 and 1, and there is a much narrower section in the middle where the model isn't confident with the output. Clearly, the logistic regression model is much better at generating binary output with a continuous input. 

There are many regression models other than linear and logistic regression that fit different data heuristics; notable ones include [decision trees](https://medium.com/@MrBam44/decision-trees-91f61a42c724), [k-nearest neighbors](https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4), and more. These models are the most rudimentary form of machine learning, and they generally work well with **simple** input and output data. We can use statistical and mathematical tools to generate a function which takes an input and spits out an output that matches as closely to the training data as possible (minimizing error). In our logistic regression example above, the regression function takes in the growth rate of the tumor and outputs the probability of the tumor being cancerous. 

However, for more complex and unstructured functions, these ideas fall short. This is where neural networks come into play. 

## Deep Learning

#### 1. Neural Networks

![Visual of handwritten digits](/assets/02-handwritten-digits.jpeg)

Neural networks use optimization algorithms to approximate complex functions (i.e. the relationship between the input variable and the output variable) that statistics and simple math cannot find. Because most neural networks have multiple hidden layers (these are explained below), they are often referred to as **deep learning**. To illustrate the basic workings of neural networks, I'll use the classic handwriting numbers recognition example â€” we have an image of a handwritten image and want the computer to determine which digit it represents. 

Assume that each input handwriting image is stored within a 25 by 25 image. The values of each of the pixels will be between somewhere between zero and one, representing the brightness of that particular pixel. If we flatten the matrix of pixels out into a vector, we now have our **inputs** to the function. Conversely, our **outputs** to the function would be the list of numbers 0 through 9. Each output would also be a numerical value between 0 and 1, representing the probability that the input image is the output digit. What we need our neural network to accomplish is to "magically" turn the 625 pixel input values into one of the 10 outputs. This sounds like a daunting task, but I promise that it is not too difficult with neural networks. 

Neurons are the heart of neural networks (as the name suggests), and they are partially inspired by the neurons in our brains. This might make neurons as a ML concept might be abstract and difficult to understand. However, to paraphrase math YouTuber [3B1B](https://www.youtube.com/@3blue1brown), when you hear the word neuron, all you need to think about is something that holds a decimal number between 0 and 1. Each neuron can be connected to other neurons. 

Our neural network in particular would start with 625 neurons, with every neuron representing a pixel in our image. The neural network would then end with 10 neurons, representing the probability that an image is one of the ten numbers. These would be our starting and ending layers of the neural network. Then, there are also an arbitrary number of layers in between the input layer and the output layer, called the **hidden layers**. Each of these would also contain an arbitrary number of neurons. All of the neurons would be connected to all of the neurons in the next and previous layers â€” forming a **fully connected neural network**. 

![Visual of a basic neural network](/assets/02-neural-networks.jpeg)

Each connection between the neurons are **weighted**, meaning the neurons in the first layer will affect the neurons in the second layer differently depending on its importance. Then, each neuron in the second layer will affect each neuron in the third, and the third layer will affect the fourth, and so on. This continues until it reaches the output layer. The premise of the neural network is that based on a large amount of data, we can use **optimization algorithms** to repeatedly adjust the weights between the neurons in the hidden layers such that the model would activate the correct output neuron based on an input. This process of adjusting the weights is known as **training**. 

In our handwriting numbers example, we would prepare a large dataset of labeled image of digits, then feed it into the neural network. The network would start out with random weights. Upon seeing each image, we would compute the error between the prediction and the actual data, then compute the derivative of the error function at the specific point. We would use this information to adjust the parameters of the model to lower the error. We would adjust the weights again and again until we have minimized the error function. This is known as the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent) optimization algorithm: 

We will, again, skip over the mathematical details here, because gradient descent and other similar DL optimization functions use complex math that is way beyond the reach of this article. However, the graphical illustration of the end result would look something like the following, where the yellow surface is the error function and the black dots represent the current error values:

![Visual of gradient descent](/assets/02-gradient-descent.gif)

In practice, we wouldn't actually know what the error function looks like without a huge amount of computation. In addition, the error function may also be in higher dimensions due to large amounts of input, such that we cannot conveniently visualize it. This is why instead of computing the entire error function, we compute the derivative of it, which is relatively computationally inexpensive, and then explore the function in the "downhill" direction. 

The product we get at the end of the training process is a neural network with a set of weights. We know what each of the weights in the hidden layers are, and we know that the set of weights produces the least error in our dataset. However, we don't exactly know what purpose each weight serves. 

In other words, we do not know why the algorithm settled on a particular set of weights, but we do know that the set of weights is the **optimal** that the training algorithm can find in order to minimize the error. In our case, the algorithms should adjust the weights of our model so that each of the corresponding numbers in the output layer should be activated when an image of the number is inputted.

![Visual of neural network](/assets/02-neural-networks.gif)

There are many special types of neural networks out in the wild, each adding new processing layers and tweaking the existing processing layers of the neural network, specializing them to become better at different tasks. For example, [Convolutional Neural Networks (CNN)](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) are neural networks that work best with image classification and special detection, [Recurrent Neural Networks (RNN)](https://umeshk1255.medium.com/everything-you-know-about-recurrent-neural-networks-rnn-part-i-e2bb74a6bf42) models good at sequential data such as text and natural language processing and machine translation. If we wanted to improve our handwriting number model, we would have applied a neural network with. 

Recently, new neural networks with the [Transformer](https://medium.com/towards-data-science/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) architecture, also specializing in text and natural language processing, have seen wide successes in overtaking RNN architectures. The architecture increases the working context window, or length of the window in which the model can "remember" data, while improving the performance of the model by parallelization. These models have become the backbone of LLMs today.

#### 2. Large Language Models

If you have made it this far into the essay-esque blog, you will have all the necessary components to learn how a large language model functions. At its core, LLMs are next word predictors using neural networks; something along the lines of a text auto completion engine on steroids using math. 

![Visual of ChatGPT](/assets/02-chatgpt.jpeg)

As alluded to earlier, the architecture of the neural network that powers the LLMs today are transformers. The transformer architecture first preprocesses the input words by breaking them into pieces known as **tokens**. These tokens are then converted into high dimensional vectors (acquired during pre-training with colossal amounts of data) that represents the rough meaning of them. The angle between the vectors represents the rough meaning of the tokens; the smaller the angle, the more similar the meaning. 

Then, the architecture applies an **attention** mechanism, where each token tweaks the other tokens in the sentence differently based on context. This makes the model much better at determining the meaning and connotations of the words depending on context. For example, the transformer model would know that the meaning of "minute" in the following sentences are different after the tweaks â€” in other words, the vector of the tokenized representation of the word "minute" would be different for the following two cases:

+ The minute hand of the clock moved
+ They did not notice the minute change

This would enable the model to understand more complex sentences and intricate word connotations. By feeding these tweaked input vectors as inputs into a **neural network**, the model generates a probability distribution of what the token word could be. The final prediction of the next token would then be selected pseudo-randomly based on the next token probabilities the model generated. The reason we don't want to always select the top prediction for the next token is because we do not want each responses to be the same given the same user input. This is done to make the responses more natural. 

By repeating this process over and over again to generate new text, we can then create a seemingly intelligent AI chat bot. When combined with various external tools, [advanced prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering#), [retrieval augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), we get the chat bots from various different companies that we know and love today.

## Conclusion

There are many other advanced techniques that one is able to use to make the LLMs sound more natural, give more accurate information, or specialize in niche areas. However, due to the length of the article and my google searching capabilities, I will not go over them here. Instead, I've left a couple of links to articles that could potentially give you a deeper understanding of AI below. In a later article (which I probably won't have the stamina to write in a while), I will discuss the ethical implications of deploying powerful AI models. In the mean time, I hope you now have a better understanding of now even though I didn't go over any math â€” "AI isn't magic, it's math".

## See also

 - [Microsoft's AI Intro Blog](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)
 - [3B1B's Neural Network Playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
 - [LLM Optimization Techniques](https://medium.com/@yugank.aman/llm-model-optimization-techniques-and-frameworks-e21d57744ca1)

Author's note: All opinions and writing presented in this article is the author's own opinion, and this article does not contain any AI generated content; AI was only used to assist with researching relevant topics. If you have any suggestions could enhance the approachability or the factual correctness of the article, feel free to leave a comment below. 
